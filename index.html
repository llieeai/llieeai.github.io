<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lujun Li | Personal Homepage</title>
    <meta content="Lujun Li, https://github/liai" name="keywords">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <style>
        :root {
            --primary-color: #1772d0;
            --secondary-color: #f09228;
            --bg-color: #f5f7fa;
            --card-bg: #ffffff;
            --text-color: #333;
            --border-color: #e0e0e0;
            --highlight: #1772d0;
            --shadow: 0 4px 20px rgba(0, 0, 0, 0.08);
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Poppins', sans-serif;
            background-color: var(--bg-color);
            color: var(--text-color);
            line-height: 1.6;
            position: relative;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        @media (max-width: 768px) {
            body {
                padding: 15px;
            }
        }

        h1, h2, h3, h4, h5, h6 {
            color: #333;
            margin: 1rem 0;
            font-weight: 600;
        }

        h2 {
            font-size: 1.8rem;
            border-bottom: 2px solid var(--highlight);
            padding-bottom: 0.3rem;
            display: inline-block;
            margin-bottom: 1.5rem;
        }

        h3 {
            font-size: 1.4rem;
        }

        h4 {
            font-size: 1.1rem;
            color: #666;
            margin-bottom: 1rem;
        }

        p, li {
            margin-bottom: 1rem;
        }

        a {
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.3s;
        }

        a:hover {
            color: var(--secondary-color);
        }

        .header {
            display: flex;
            align-items: center;
            gap: 2rem;
            margin-bottom: 2rem;
            padding: 1.5rem;
            background-color: var(--card-bg);
            border-radius: 12px;
            box-shadow: var(--shadow);
        }

        .profile-image {
            width: 150px;
            height: 150px;
            border-radius: 50%;
            object-fit: cover;
            border: 3px solid var(--highlight);
        }

        .profile-info {
            flex-grow: 1;
        }

        .profile-name {
            font-size: 2rem;
            margin-bottom: 0.5rem;
            color: #333;
        }

        .profile-title {
            font-size: 1.1rem;
            color: #666;
            margin-bottom: 1rem;
        }

        .contact-info {
            margin-top: 1rem;
        }

        .contact-info p {
            margin-bottom: 0.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .contact-info i {
            color: var(--highlight);
            width: 20px;
        }

        .social-links {
            display: flex;
            gap: 1rem;
            margin-top: 1rem;
        }

        .social-links a {
            display: inline-flex;
            align-items: center;
            justify-content: center;
            width: 36px;
            height: 36px;
            border-radius: 50%;
            background-color: var(--highlight);
            color: white;
            transition: all 0.3s;
        }

        .social-links a:hover {
            background-color: var(--secondary-color);
            transform: translateY(-3px);
        }

        .section {
            margin-bottom: 2.5rem;
        }

        .card {
            background-color: var(--card-bg);
            border-radius: 12px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            box-shadow: var(--shadow);
        }

        .paper {
            display: flex;
            gap: 1.5rem;
            margin-bottom: 2rem;
            position: relative;
            background-color: var(--card-bg);
            border-radius: 12px;
            padding: 1.5rem;
            box-shadow: var(--shadow);
        }

        .paper-image {
            width: 200px;
            height: 120px;
            object-fit: cover;
            border-radius: 8px;
            flex-shrink: 0;
        }

        .paper-content {
            flex-grow: 1;
        }

        .paper-title {
            font-size: 1.2rem;
            font-weight: 600;
            margin-bottom: 0.5rem;
        }

        .paper-authors {
            margin-bottom: 0.5rem;
        }

        .paper-venue {
            margin-bottom: 0.5rem;
            font-style: italic;
        }

        .paper-alert {
            color: #e74c3c;
            font-weight: 500;
            margin-bottom: 0.5rem;
        }

        .paper-links {
            display: flex;
            gap: 1rem;
            margin-top: 0.5rem;
        }

        .paper-links a {
            padding: 0.3rem 0.8rem;
            background-color: #f5f7fa;
            border-radius: 4px;
            font-size: 0.9rem;
            transition: all 0.3s;
        }

        .paper-links a:hover {
            background-color: #e0e5ed;
        }

        .paper-abstract {
            margin-top: 0.5rem;
            font-size: 0.95rem;
        }

        ul {
            list-style: none;
            padding-left: 1.5rem;
        }

        ul li {
            position: relative;
            margin-bottom: 0.8rem;
        }

        ul li::before {
            content: "•";
            color: var(--highlight);
            font-weight: bold;
            display: inline-block;
            width: 1em;
            margin-left: -1em;
        }
        
        @media (max-width: 768px) {
            .header {
                flex-direction: column;
                text-align: center;
                gap: 1rem;
            }
            
            .profile-image {
                width: 120px;
                height: 120px;
            }
            
            .social-links {
                justify-content: center;
            }
            
            .paper {
                flex-direction: column;
            }
            
            .paper-image {
                width: 100%;
                height: auto;
                max-height: 180px;
            }
        }

        .highlight {
            color: var(--highlight);
            font-weight: 600;
        }

        .news-item {
            margin-bottom: 0.8rem;
            display: flex;
        }

        .news-date {
            font-weight: 600;
            color: var(--highlight);
            min-width: 100px;
        }

        .map-container {
            height: 350px;
            width: 100%;
            margin-top: 1rem;
            overflow: hidden;
            border-radius: 12px;
        }

        .visitor-info {
            margin-top: 1rem;
            display: flex;
            gap: 1.5rem;
            flex-wrap: wrap;
        }

        .visitor-card {
            background-color: var(--card-bg);
            border-radius: 8px;
            padding: 1rem;
            flex-grow: 1;
            min-width: 150px;
            box-shadow: var(--shadow);
            text-align: center;
        }

        .visitor-value {
            font-size: 1.8rem;
            font-weight: 600;
            color: var(--highlight);
        }

        .visitor-label {
            font-size: 0.9rem;
            color: #666;
        }
    </style>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164510176-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-164510176-1');
    </script>
</head>

<body>
    <header class="header">
        <img class="profile-image" src="./lujunli.jpg" alt="Lujun Li">
        <div class="profile-info">
            <h1 class="profile-name">Lujun LI (李路军)</h1>
            <p class="profile-title">Ph.D. candidate in Hong Kong University of Science and Technology (HKUST)</p>

            <div class="contact-info">
                <p><i class="fas fa-map-marker-alt"></i> Clear Water Bay Peninsula, New Territories, Hong Kong</p>
                <p><i class="fas fa-envelope"></i> lilujunai@gmail.com, lliee@ust.hk</p>
            </div>

            <div class="social-links">
                <a href="https://openreview.net/profile?id=~Lujun_Li1" title="OpenReview"><i class="fas fa-file-alt"></i></a>
                <a href="https://scholar.google.com/citations?hl=en&user=aPl3DjIAAAAJ&view_op=list_works&gmla=AJsN-F6PzWqNjlzy8OapYAJ7NBWh_vlvN_-G0boZwefvxSkVZAlxhoFnfcO7yhvjQzM8qSmFKDsaTnULJLHvRzDpIqfgabOt9FmyBOaHz_hcjDMdRy47Ois" title="Google Scholar"><i class="fas fa-graduation-cap"></i></a>
                <a href="https://github.com/lliai" title="GitHub"><i class="fab fa-github"></i></a>
                <a href="https://twitter.com/luking66" title="Twitter"><i class="fab fa-twitter"></i></a>
            </div>
        </div>
    </header>



    <div class="section">
        <h2>About Me</h2>
        <div class="card">
            <p>I am currently a final-year Ph.D. candidate in HKUST, supervised by Prof. <a href="https://cse.hkust.edu.hk/admin/people/faculty/profile/yikeguo">Yi-Ke Guo</a> (Provost & Fellow of REng|HKEng|IEEE).</p>
            
            <p>My research focuses on advancing <strong class="highlight">Efficient Machine Learning and Large Language Models </strong>  to make AGI smaller, faster, greener and cheaper via novel compression techniques:</p>
            
            <p><strong class="highlight">Efficient Mixture-of-Experts (MoE):</strong> MoE-SVD  (ICML'25), D2-MoE  (ICML'25), Sub-MoE (AAAI'26)</p>
            <p><strong class="highlight">Efficient LLM Fine-tuning:</strong> NoRA (ICCV'25), AIRA (ICCV'25)</p>
            <p><strong class="highlight">Efficient  Large Language Models:</strong> Pruner-Zero (ICML'24), DSA (NeurIPS'24), ALS (NeurIPS'24), STBLLM (ICLR'25)</p>
            
            <p><strong class="highlight">Automated Efficient ML:</strong> EMQ (ICCV'23), ParZC (AAAI'25), AutoProx (AAAI'23), SasWOT (AAAI'23), Auto-GAS (ECCV'24), AttnZero (ECCV'24)</p>
            
            <p><strong class="highlight">Automated Distillation:</strong> DisWOT (CVPR'23), Auto-KD (ICCV'23), KD-Zero (NeurIPS'23), DetKDS (ICML'24), Auto-DAS (ECCV'24)</p>
            
            <p><strong class="highlight">Knowledge Distillation:</strong> Tf-FD (ECCV'22), SHAKE (NeurIPS'22), NORM (ICLR'23)</p>
            <p>I am/was the Area Chair for ICLR'25, NeurIPS'25, ACM-MM'25, BMCV'24 and more. I am awarded the Ant InTech Scholarship–Future and DAAD NeT-AI Fellowship-2024. </p>

            
           
               <p style="color:#e74c3c; font-weight: 600; margin-top: 1rem;"> I'll be at AAAI 26 in Singapore (Jan 20 – 27th) in person—happy to chat!</p>
                  <p> News: Two papers accepted by AAAI'26 and One papers accepted by ACM-MM'25 (on  MoE and Merged LLMs ): </p>
                    <p> News: Two papers accepted by ICCV'25 and there papers accepted by ICML'25 (on Efficient MoE LLMs): </p>
                 <p> News: Two papers accepted by ICCV'25 and there papers accepted by ICML'25 (on Efficient MoE LLMs): </p>
             <p> Selected Preprints: </p>
            <div class="news-item">
                <div class="news-date">2025-08</div>
                <div><a href="https://arxiv.org/abs/2508.16134">CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing</a></div>
            </div>
             <div class="news-item">
                <div class="news-date">2025-06</div>
                <div><a href="https://arxiv.org/abs/2506.12040">BTC-LLM: Efficient Sub-1-Bit LLM Quantization via Learnable Transformation and Binary Codebook</a></div>
            </div>            
        </div>
    </div>


<div class="section">
    <h2>First-author Publications (22)</h2>
    <h4>*: Co-first author, **: Corresponding author or project leadership</h4>

    <div class="filter-container" style="display: flex; flex-direction: column; align-items: center; margin-bottom: 2rem; background-color: #f9f9f9; padding: 1.5rem; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.05);">
        <h3 style="margin-bottom: 1rem; color: #1772d0; text-align: center;">Efficient Machine Learning and Large Language Models</h3>
        
        <div class="publication-year-filter" style="display: flex; flex-wrap: wrap; justify-content: center; gap: 0.5rem; margin-bottom: 1.2rem;">
            <button class="year-btn active" data-year="all" style="padding: 0.5rem 1rem; background-color: #1772d0; color: white; border: none; border-radius: 20px; cursor: pointer; font-weight: 500; transition: all 0.3s; min-width: 80px;">All Years</button>
            <button class="year-btn" data-year="2025" style="padding: 0.5rem 1rem; background-color: #f5f7fa; color: #333; border: 1px solid #e0e0e0; border-radius: 20px; cursor: pointer; font-weight: 500; transition: all 0.3s; min-width: 80px;">2025</button>
            <button class="year-btn" data-year="2024" style="padding: 0.5rem 1rem; background-color: #f5f7fa; color: #333; border: 1px solid #e0e0e0; border-radius: 20px; cursor: pointer; font-weight: 500; transition: all 0.3s; min-width: 80px;">2024</button>
            <button class="year-btn" data-year="2023" style="padding: 0.5rem 1rem; background-color: #f5f7fa; color: #333; border: 1px solid #e0e0e0; border-radius: 20px; cursor: pointer; font-weight: 500; transition: all 0.3s; min-width: 80px;">2023</button>
            <button class="year-btn" data-year="2022" style="padding: 0.5rem 1rem; background-color: #f5f7fa; color: #333; border: 1px solid #e0e0e0; border-radius: 20px; cursor: pointer; font-weight: 500; transition: all 0.3s; min-width: 80px;">2022</button>
        </div>

        <div class="publications-filter" style="display: flex; flex-wrap: wrap; justify-content: center; gap: 0.5rem;">
            <button class="filter-btn active" data-filter="all" style="padding: 0.5rem 1rem; background-color: #1772d0; color: white; border: none; border-radius: 20px; cursor: pointer; font-weight: 500; transition: all 0.3s;">All</button>
            <button class="filter-btn" data-filter="neurips" style="padding: 0.5rem 1rem; background-color: #f5f7fa; color: #333; border: 1px solid #e0e0e0; border-radius: 20px; cursor: pointer; font-weight: 500; transition: all 0.3s;">NeurIPS <span style="display: inline-flex; align-items: center; justify-content: center; background-color: #1772d0; color: white; border-radius: 12px; padding: 0.1rem 0.5rem; font-size: 0.8rem; margin-left: 0.3rem;">4</span></button>
            <button class="filter-btn" data-filter="icml" style="padding: 0.5rem 1rem; background-color: #f5f7fa; color: #333; border: 1px solid #e0e0e0; border-radius: 20px; cursor: pointer; font-weight: 500; transition: all 0.3s;">ICML <span style="display: inline-flex; align-items: center; justify-content: center; background-color: #1772d0; color: white; border-radius: 12px; padding: 0.1rem 0.5rem; font-size: 0.8rem; margin-left: 0.3rem;">4</span></button>
            <button class="filter-btn" data-filter="iclr" style="padding: 0.5rem 1rem; background-color: #f5f7fa; color: #333; border: 1px solid #e0e0e0; border-radius: 20px; cursor: pointer; font-weight: 500; transition: all 0.3s;">ICLR <span style="display: inline-flex; align-items: center; justify-content: center; background-color: #1772d0; color: white; border-radius: 12px; padding: 0.1rem 0.5rem; font-size: 0.8rem; margin-left: 0.3rem;">2</span></button>
            <button class="filter-btn" data-filter="cvpr" style="padding: 0.5rem 1rem; background-color: #f5f7fa; color: #333; border: 1px solid #e0e0e0; border-radius: 20px; cursor: pointer; font-weight: 500; transition: all 0.3s;">CVPR <span style="display: inline-flex; align-items: center; justify-content: center; background-color: #1772d0; color: white; border-radius: 12px; padding: 0.1rem 0.5rem; font-size: 0.8rem; margin-left: 0.3rem;">1</span></button>
            <button class="filter-btn" data-filter="iccv" style="padding: 0.5rem 1rem; background-color: #f5f7fa; color: #333; border: 1px solid #e0e0e0; border-radius: 20px; cursor: pointer; font-weight: 500; transition: all 0.3s;">ICCV <span style="display: inline-flex; align-items: center; justify-content: center; background-color: #1772d0; color: white; border-radius: 12px; padding: 0.1rem 0.5rem; font-size: 0.8rem; margin-left: 0.3rem;">4</span></button>
            <button class="filter-btn" data-filter="eccv" style="padding: 0.5rem 1rem; background-color: #f5f7fa; color: #333; border: 1px solid #e0e0e0; border-radius: 20px; cursor: pointer; font-weight: 500; transition: all 0.3s;">ECCV <span style="display: inline-flex; align-items: center; justify-content: center; background-color: #1772d0; color: white; border-radius: 12px; padding: 0.1rem 0.5rem; font-size: 0.8rem; margin-left: 0.3rem;">4</span></button>
            <button class="filter-btn" data-filter="aaai" style="padding: 0.5rem 1rem; background-color: #f5f7fa; color: #333; border: 1px solid #e0e0e0; border-radius: 20px; cursor: pointer; font-weight: 500; transition: all 0.3s;">AAAI <span style="display: inline-flex; align-items: center; justify-content: center; background-color: #1772d0; color: white; border-radius: 12px; padding: 0.1rem 0.5rem; font-size: 0.8rem; margin-left: 0.3rem;">3</span></button>
        </div>
    </div>



        <div class="paper" data-year="2025" data-venue="iccv">
        <img class="paper-image" src="./aira.png" alt="AIRA: Activation-Informed Low-Rank Adaptation for Large Models">
        <div class="paper-content">
            <div class="paper-title">AIRA: Activation-Informed Low-Rank Adaptation for Large Models</div>
            <div class="paper-authors"><span class="highlight">Lujun Li</span>, Dezhi_Li, Cheng Lin, Wei Li, Wei Xue, Sirui Han, Yike Guo </div>
            <div class="paper-venue">International Conference on Computer Vision (ICCV-2025)</div>
            <div class="paper-alert">CCF-A, Top Conference in Computer Vision</div>
            <div class="paper-links">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Li_AIRA_Activation-Informed_Low-Rank_Adaptation_for_Large_Models_ICCV_2025_paper.pdf">Paper</a>
                <a href="https://github.com/lliai/LoRA-Zoo">Code</a>
            </div>
            <div class="paper-abstract">AIRA introduces: (1) Outlier-weighted SVD  initialization, (2) Outlier-driven dynamic rank assignment, and (3) Activation-informed training.</div>
        </div>
    </div>

           <div class="paper" data-year="2025" data-venue="iccv">
        <img class="paper-image" src="./nora.png" alt="Efficient Fine-Tuning of Large Models via Nested Low-Rank Adaptation">
        <div class="paper-content">
            <div class="paper-title">Efficient Fine-Tuning of Large Models via Nested Low-Rank Adaptation</div>
            <div class="paper-authors"><span class="highlight">Lujun Li</span>, Cheng Lin, Dezhi Li, You-Liang Huang, Wei Li, Tianyu Wu, Jie Zou, Wei Xue, Sirui Han, Yike Guo </div>
            <div class="paper-venue">International Conference on Computer Vision (ICCV-2025)</div>
            <div class="paper-alert">CCF-A, Top Conference in Computer Vision</div>
            <div class="paper-links">
                <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Li_AIRA_Activation-Informed_Low-Rank_Adaptation_for_Large_Models_ICCV_2025_paper.pdf">Paper</a>
                <a href="https://github.com/lliai/LoRA-Zoo">Code</a>
            </div>
            <div class="paper-abstract">We present NoRA, a novel nested parameter-efficient LoRA structure, optimizes large model fine-tuning by employing serial structures.</div>
        </div>
    </div>

    

    
        <div class="paper" data-year="2025" data-venue="icml">
        <img class="paper-image" src="./moe-svd.png" alt="MoE-SVD: Structured Mixture-of-Experts LLMs Compression via Singular Value Decomposition">
        <div class="paper-content">
            <div class="paper-title">MoE-SVD: Structured Mixture-of-Experts LLMs Compression via Singular Value Decomposition</div>
            <div class="paper-authors">Wei Li, <span class="highlight">Lujun Li*</span>, Hao Gu, You-Liang Huang, Mark G. Lee, Shengjie Sun, Wei Xue, Yike Guo</div>
            <div class="paper-venue">Forty-second International Conference on Machine Learning (ICML-2025)</div>
            <div class="paper-alert">CCF-A, Top Conference in Artificial Intelligence</div>
            <div class="paper-links">
                <a href="https://openreview.net/forum?id=ho7ZUS1z8A">Paper</a>
                <a href="https://openreview.net/forum?id=ho7ZUS1z8A">Code</a>
            </div>
            <div class="paper-abstract">In this paper, we present a novel training-free compressor for MoE LLMs that uses SVD to break experts into smaller matrices, sharing and trimming them to save memory, speed up inference.</div>
        </div>
    </div>

        <div class="paper" data-year="2025" data-venue="icml">
        <img class="paper-image" src="./d2-moe.png" alt="Delta Decompression for MoE-based LLMs Compression">
        <div class="paper-content">
            <div class="paper-title">Delta Decompression for MoE-based LLMs Compressionn</div>
            <div class="paper-authors"> Hao Gu, Wei Li, <span class="highlight">Lujun Li*</span>, Zhu Qiyuan, Mark G. Lee, Shengjie Sun, Wei Xue, Yike Guo</div>
            <div class="paper-venue">Forty-second International Conference on Machine Learning (ICML-2025)</div>
            <div class="paper-alert">CCF-A, Top Conference in Artificial Intelligence</div>
            <div class="paper-links">
                <a href="https://arxiv.org/abs/2502.17298">Paper</a>
                <a href="https://github.com/lliai/D2MoE">Code</a>
            </div>
            <div class="paper-abstract">In this paper, we present D2-MoE, a new delta decompression  MoE compressor that decomposes expert weights into a shared base weight and unique delta weights..</div>
        </div>
    </div>
    
    <div class="paper" data-year="2025" data-venue="iclr">
        <img class="paper-image" src="./stbllm.png" alt="STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs">
        <div class="paper-content">
            <div class="paper-title">STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs</div>
            <div class="paper-authors">Peijie Dong, <span class="highlight">Lujun Li*</span>, Yuedong Zhong, DaYou Du, Ruibo FAN, Yuhan Chen, Zhenheng Tang, Qiang Wang, Wei Xue, Yike Guo, Xiaowen Chu</div>
            <div class="paper-venue">The Thirteenth International Conference on Learning Representations (ICLR-2025)</div>
            <div class="paper-alert">THU-A, Top Conference in Artificial Intelligence</div>
            <div class="paper-links">
                <a href="https://arxiv.org/abs/2408.01803">Paper</a>
                <a href="https://arxiv.org/abs/2408.01803">Code</a>
            </div>
            <div class="paper-abstract">We introduces STBLLM, a novel approach that breaks the 1-bit barrier in language models by leveraging Structured Binary LLMs.</div>
        </div>
    </div>

    <div class="paper" data-year="2025" data-venue="aaai">
        <img class="paper-image" src="./parzc.png" alt="ParZC: Parametric Zero-Cost Proxies for Efficient NAS">
        <div class="paper-content">
            <div class="paper-title">ParZC: Parametric Zero-Cost Proxies for Efficient NAS</div>
            <div class="paper-authors">Peijie Dong, <span class="highlight">Lujun Li*</span>, Zhenheng Tang, Zimian Wei, Xiang Liu, Qiang Wang, Xiaowen Chu</div>
            <div class="paper-venue">Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-2025)</div>
            <div class="paper-alert">CCF-A, Top Conference in Artificial Intelligence</div>
            <div class="paper-links">
                <a href="https://arxiv.org/abs/2402.02105">Paper</a>
                <a href="https://arxiv.org/abs/2402.02105">Code</a>
            </div>
            <div class="paper-abstract">Our Parametric Zero-Cost Proxies (ParZC) improves zero-shot Neural Architecture Search by addressing unequal node importance and using novel techniques for uncertainty estimation and architecture ranking.</div>
        </div>
    </div>

    <div class="paper" data-year="2024" data-venue="neurips">
        <img class="paper-image" src="./DSA.png" alt="Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models">
        <div class="paper-content">
            <div class="paper-title">Discovering Sparsity Allocation for Layer-wise Pruning of Large Language Models</div>
            <div class="paper-authors"><span class="highlight">Lujun Li</span>, Peijie Dong, Zhenheng Tang, Xiang Liu, Qiang Wang, Wenhan Luo, Wei Xue, Qifeng Liu, Xiaowen Chu, Yike Guo</div>
            <div class="paper-venue">Conference on Neural Information Processing Systems (NeurIPS-2024)</div>
            <div class="paper-alert">CCF-A, Top Conference in Machine Learning</div>
            <div class="paper-links">
                <a href="https://openreview.net/forum?id=rgtrYVC9n4">Paper</a>
                <a href="https://github.com/lliai/DSA">Code</a>
            </div>
            <div class="paper-abstract">In this paper, we present DSA, the first automated framework for discovering sparsity allocation schemes for layer-wise pruning in Large Language Models.</div>
        </div>
    </div>
    
    <div class="paper" data-year="2024" data-venue="neurips">
        <img class="paper-image" src="./ALS.jpg" alt="Adaptive Layer Sparsity for Large Language Models via Activation Correlation Assessment">
        <div class="paper-content">
            <div class="paper-title">Adaptive Layer Sparsity for Large Language Models via Activation Correlation Assessment</div>
            <div class="paper-authors">Wei Li, <span class="highlight">Lujun Li**</span>, Mark G. Lee, Shengjie Sun</div>
            <div class="paper-venue">Conference on Neural Information Processing Systems (NeurIPS-2024)</div>
            <div class="paper-alert">CCF-A, Top Conference in Machine Learning</div>
            <div class="paper-links">
                <a href="https://openreview.net/forum?id=Jup0qZxH7U">Paper</a>
                <a href="https://github.com/lliai/ALS">Code</a>
            </div>
            <div class="paper-abstract">In this paper, we present an approach called Adaptive Layer Sparsity for optimizing large language models by selectively pruning features in intermediate layers.</div>
        </div>
    </div>
    
    <div class="paper" data-year="2024" data-venue="icml">
        <img class="paper-image" src="./PrunerZero.png" alt="Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models">
        <div class="paper-content">
            <div class="paper-title">Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models</div>
            <div class="paper-authors">Peijie Dong, <span class="highlight">Lujun Li*</span>, Zhenheng Tang, Xiang Liu, Xinglin Pan, Qiang Wang, Xiaowen Chu</div>
            <div class="paper-venue">International Conference on Machine Learning (ICML-2024)</div>
            <div class="paper-alert">CCF-A, Top Conference in Machine Learning</div>
            <div class="paper-links">
                <a href="https://openreview.net/pdf?id=1tRLxQzdep">Paper</a>
                <a href="https://github.com/pprp/Pruner-Zero">Code</a>
            </div>
            <div class="paper-abstract">We propose Pruner-Zero framework to automatically devise pruning metrics for post-training pruning LLMs.</div>
        </div>
    </div>
    
    <div class="paper" data-year="2024" data-venue="icml">
        <img class="paper-image" src="./DetKDS.png" alt="DetKDS: Knowledge Distillation Search for Object Detectors">
        <div class="paper-content">
            <div class="paper-title">DetKDS: Knowledge Distillation Search for Object Detectors</div>
            <div class="paper-authors"><span class="highlight">Lujun Li</span>, Yufan Bao, Peijie Dong, Chuanguang Yang, Anggeng Li, Wenhan Luo, Qifeng Liu, Wei Xue, Yike Guo</div>
            <div class="paper-venue">International Conference on Machine Learning (ICML-2024)</div>
            <div class="paper-alert">CCF-A, Top Conference in Machine Learning</div>
            <div class="paper-links">
                <a href="https://openreview.net/pdf/fcca99569d2c5ab0653814d94a6625b2e27ef2a2.pdf">Paper</a>
                <a href="https://github.com/lliai/DetKDS">Code</a>
            </div>
            <div class="paper-abstract">In this paper, we present DetKDS, the first knowledge distillation search framework to enhance any detectors by searching for optimal distillation policies.</div>
        </div>
    </div>
    
    <div class="paper" data-year="2024" data-venue="eccv">
        <img class="paper-image" src="./AttnZero.png" alt="AttnZero: Efficient Attention Discovery for Vision Transformers">
        <div class="paper-content">
            <div class="paper-title">AttnZero: Efficient Attention Discovery for Vision Transformers</div>
            <div class="paper-authors"><span class="highlight">Lujun Li</span>, Zimian Wei, Peijie Dong, Wenhan Luo, Wei Xue, Qifeng Liu, Yike Guo</div>
            <div class="paper-venue">European Conference on Computer Vision (ECCV-2024)</div>
            <div class="paper-alert">Top Conference in Computer Vision</div>
            <div class="paper-links">
                <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00666.pdf">Paper</a>
                <a href="https://github.com/lliai/AttnZero">Code</a>
            </div>
            <div class="paper-abstract">In this paper, we present AttnZero, the first framework for automatically discovering efficient attention modules tailored for Vision Transformers (ViTs).</div>
        </div>
    </div>
    
    <div class="paper" data-year="2024" data-venue="eccv">
        <img class="paper-image" src="./AutoGAS.png" alt="Auto-GAS: Automated Proxy Discovery for Training-free Generative Architecture Search">
        <div class="paper-content">
            <div class="paper-title">Auto-GAS: Automated Proxy Discovery for Training-free Generative Architecture Search</div>
            <div class="paper-authors"><span class="highlight">Lujun Li</span>, Haosen Sun, Shiwen Li, Peijie Dong, Wenhan Luo, Wei Xue, Qifeng Liu, Yike Guo</div>
            <div class="paper-venue">European Conference on Computer Vision (ECCV-2024)</div>
            <div class="paper-alert">Top Conference in Computer Vision</div>
            <div class="paper-links">
                <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00668.pdf">Paper</a>
                <a href="https://github.com/lliai/Auto-GAS">Code</a>
            </div>
            <div class="paper-abstract">In this paper, we introduce Auto-GAS, the first training-free Generative Architecture Search (GAS) framework enabled by an auto-discovered proxy.</div>
        </div>
    </div>
    
    <div class="paper" data-year="2024" data-venue="eccv">
        <img class="paper-image" src="./AutoDAS.png" alt="Auto-DAS: Automated Proxy Discovery for Training-free Distillation-aware Architecture Search">
        <div class="paper-content">
            <div class="paper-title">Auto-DAS: Automated Proxy Discovery for Training-free Distillation-aware Architecture Search</div>
            <div class="paper-authors">Haosen Sun, <span class="highlight">Lujun Li</span>, Peijie Dong, Zimian Wei, Shitong Shao</div>
            <div class="paper-venue">European Conference on Computer Vision (ECCV-2024)</div>
            <div class="paper-alert">Top Conference in Computer Vision</div>
            <div class="paper-links">
                <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00676.pdf">Paper</a>
                <a href="https://github.com/lliai/Auto-DAS">Code</a>
            </div>
            <div class="paper-abstract">In this paper, we present Auto-DAS, an automatic proxy discovery framework using an Evolutionary Algorithm (EA) for training-free DAS.</div>
        </div>
    </div>
    
    <div class="paper" data-year="2023" data-venue="neurips">
        <img class="paper-image" src="./kd-zero.png" alt="KD-Zero: Evolving Knowledge Distiller for Any Teacher-Student Pairs">
        <div class="paper-content">
            <div class="paper-title">KD-Zero: Evolving Knowledge Distiller for Any Teacher-Student Pairs</div>
            <div class="paper-authors"><span class="highlight">Lujun Li</span>, Peijie Dong, Anggeng Li, Zimian Wei, Ya Yang</div>
            <div class="paper-venue">Conference on Neural Information Processing Systems (NeurIPS-2023)</div>
            <div class="paper-alert">CCF-A, Top Conference in Machine Learning</div>
            <div class="paper-links">
                <a href="https://openreview.net/pdf?id=OlMKa5YZ8e">Paper</a>
                <a href="https://openreview.net/pdf?id=OlMKa5YZ8e">Code</a>
            </div>
            <div class="paper-abstract">We present KD-Zero, the first auto-search framework for evolving best distiller from scratch to alleviate teacher-student gaps.</div>
        </div>
    </div>
    
    <div class="paper" data-year="2023" data-venue="iccv">
        <img class="paper-image" src="./autokd.png" alt="Automated Knowledge Distillation via Monte Carlo Tree Search">
        <div class="paper-content">
            <div class="paper-title">Automated Knowledge Distillation via Monte Carlo Tree Search</div>
            <div class="paper-authors"><span class="highlight">Lujun Li</span>, Peijie Dong, Zimian Wei, Ya Yang</div>
            <div class="paper-venue">International Conference on Computer Vision (ICCV-2023)</div>
            <div class="paper-alert">CCF-A, Top Conference in Computer Vision</div>
            <div class="paper-links">
                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Automated_Knowledge_Distillation_via_Monte_Carlo_Tree_Search_ICCV_2023_paper.pdf">Paper</a>
                <a href="https://github.com/liai/Auto-KD">Code</a>
            </div>
            <div class="paper-abstract">In this paper, we present Auto-KD, the first automated search framework for optimal knowledge distillation design.</div>
        </div>
    </div>
    
    <div class="paper" data-year="2023" data-venue="iccv">
        <img class="paper-image" src="./EMQ.png" alt="EMQ: Evolving Training-free Proxies for Automated Mixed Precision Quantization">
        <div class="paper-content">
            <div class="paper-title">EMQ: Evolving Training-free Proxies for Automated Mixed Precision Quantization</div>
            <div class="paper-authors">Peijie Dong, <span class="highlight">Lujun Li*</span>, Zimian Wei, Xin Niu, Zhiliang Tian, Hengyue Pan</div>
            <div class="paper-venue">International Conference on Computer Vision (ICCV-2023)</div>
            <div class="paper-alert">CCF-A, Top Conference in Computer Vision</div>
            <div class="paper-links">
                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Dong_EMQ_Evolving_Training-free_Proxies_for_Automated_Mixed_Precision_Quantization_ICCV_2023_paper.pdf">Paper</a>
                <a href="https://github.com/liai/EMQ-series">Code</a>
            </div>
            <div class="paper-abstract">We first build the MQ-Bench-101 and develop an automatic search of proxies framework for MQ via evolving algorithms.</div>
        </div>
    </div>
    
    <div class="paper" data-year="2024" data-venue="aaai">
        <img class="paper-image" src="./autoprox.png" alt="Auto-Prox: Training-Free Vision Transformer Architecture Search via Automatic Proxy Discovery">
        <div class="paper-content">
            <div class="paper-title">Auto-Prox: Training-Free Vision Transformer Architecture Search via Automatic Proxy Discovery</div>
            <div class="paper-authors">Zimian Wei, <span class="highlight">Lujun Li*</span>, Peijie Dong, Zheng Hui, Anggeng Li, Menglong Lu, Hengyue Pan, Dongsheng Li</div>
            <div class="paper-venue">Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-2024)</div>
            <div class="paper-alert">CCF-A, Top Conference in Artificial Intelligence</div>
            <div class="paper-links">
                <a href="https://arxiv.org/abs/2402.02105">Paper</a>
                <a href="https://github.com/lliai/Auto-Prox-AAAI24">Code</a>
            </div>
            <div class="paper-abstract">We first build the ViT-Bench-101 and develop zero-cost proxy search for Vision Transformer on multiple datasets.</div>
        </div>
    </div>
    
    <div class="paper" data-year="2024" data-venue="aaai">
        <img class="paper-image" src="./saswot.png" alt="SasWOT: Real-time Semantic Segmentation Architecture Search WithOut Training">
        <div class="paper-content">
            <div class="paper-title">SasWOT: Real-time Semantic Segmentation Architecture Search WithOut Training</div>
            <div class="paper-authors">Chendi Zhu, <span class="highlight">Lujun Li*</span>, Yuli Wu, Zheng Hui, Zhengxing Sun</div>
            <div class="paper-venue">Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-2024)</div>
            <div class="paper-alert">CCF-A, Top Conference in Artificial Intelligence</div>
            <div class="paper-links">
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28606/29177">Paper</a>
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28606/29177">Code</a>
            </div>
            <div class="paper-abstract">We present the first training-free architecture search framework for Real-time Semantic Segmentation.</div>
        </div>
    </div>
    
    <div class="paper" data-year="2022" data-venue="neurips">
        <img class="paper-image" src="./shake.png" alt="Shadow Knowledge Distillation: Bridging Offline and Online Knowledge Transfer">
        <div class="paper-content">
            <div class="paper-title">Shadow Knowledge Distillation: Bridging Offline and Online Knowledge Transfer</div>
            <div class="paper-authors"><span class="highlight">Lujun Li</span>, Zhe Jin</div>
            <div class="paper-venue">Conference on Neural Information Processing Systems (NeurIPS-2022)</div>
            <div class="paper-alert">CCF-A, Top Conference in Machine Learning</div>
            <div class="paper-links">
                <a href="https://openreview.net/pdf?id=prQT0gN81oG">Paper</a>
                <a href="https://github.com/liai/SHAKE">Code</a>
            </div>
            <div class="paper-abstract">We present SHAKE with reversed distillation and shadow head to bridge offline and online knowledge transfer, achieving superior performance in multiple tasks and scenarios.</div>
        </div>
    </div>
    
    <div class="paper" data-year="2022" data-venue="eccv">
        <img class="paper-image" src="./tffd.png" alt="Self-Regulated Feature Learning via Teacher-free Feature Distillation">
        <div class="paper-content">
            <div class="paper-title">Self-Regulated Feature Learning via Teacher-free Feature Distillation</div>
            <div class="paper-authors"><span class="highlight">Lujun Li</span></div>
            <div class="paper-venue">European Conference on Computer Vision (ECCV-2022)</div>
            <div class="paper-alert">CCF-B, Top Conference in Computer Vision</div>
            <div class="paper-links">
                <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136860337.pdf">Paper</a>
                <a href="https://github.com/liai/Teacher-free-Distillation">Code</a>
            </div>
            <div class="paper-abstract">We propose Tf-FD for reusing channel-wise and layer-wise meaningful features within the student to provide teacher-like knowledge without an additional model.</div>
        </div>
    </div>
    
    <div class="paper" data-year="2023" data-venue="cvpr">
        <img class="paper-image" src="./diswot.png" alt="DisWOT: Student Architecture Search for Distillation WithOut Training">
        <div class="paper-content">
            <div class="paper-title">DisWOT: Student Architecture Search for Distillation WithOut Training</div>
            <div class="paper-authors">Peijie Dong, <span class="highlight">Lujun Li***</span>, Zimian Wei</div>
            <div class="paper-venue">IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR-2023)</div>
            <div class="paper-alert">CCF-A, Top Conference in Computer Vision</div>
            <div class="paper-links">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_DisWOT_Student_Architecture_Search_for_Distillation_WithOut_Training_CVPR_2023_paper.pdf">Paper</a>
                <a href="https://github.com/liai/DisWOT-CVPR2023">Code</a>
            </div>
            <div class="paper-abstract">We propose a strong self-supervised augmented knowledge distillation method from hierarchical feature maps for image classification.</div>
        </div>
    </div>
    
    <div class="paper" data-year="2023" data-venue="iclr">
        <img class="paper-image" src="./norm.png" alt="NORM: Knowledge Distillation via N-to-One Representation Matching">
        <div class="paper-content">
            <div class="paper-title">NORM: Knowledge Distillation via N-to-One Representation Matching</div>
            <div class="paper-authors">Xiaolong Liu, <span class="highlight">Lujun Li***</span>, Chao Li, Anbang Yao</div>
            <div class="paper-venue">International Conference on Learning Representations (ICLR-2023)</div>
            <div class="paper-alert">Top Conference in Machine Learning</div>
            <div class="paper-links">
                <a href="https://openreview.net/pdf?id=CRNwGauQpb6">Paper</a>
                <a href="https://openreview.net/attachment?id=CRNwGauQpb6&name=supplementary_material">Code</a>
            </div>
            <div class="paper-abstract">We presents a new knowledge distillation method via n-to-one representation matching.</div>
        </div>
    </div>
</div>

<script>
// JavaScript for the publication filters
document.addEventListener('DOMContentLoaded', function() {
    // Year filter functionality
    const yearButtons = document.querySelectorAll('.year-btn');
    yearButtons.forEach(button => {
        button.addEventListener('click', function() {
            // Remove active class from all year buttons
            yearButtons.forEach(btn => {
                btn.classList.remove('active');
                btn.style.backgroundColor = '#f5f7fa';
                btn.style.color = '#333';
                btn.style.border = '1px solid #e0e0e0';
            });
            
            // Add active class to clicked button
            this.classList.add('active');
            this.style.backgroundColor = '#1772d0';
            this.style.color = 'white';
            this.style.border = 'none';
            
            const year = this.getAttribute('data-year');
            const papers = document.querySelectorAll('.paper');
            
            papers.forEach(paper => {
                if (year === 'all' || paper.getAttribute('data-year') === year) {
                    paper.style.display = 'flex';
                } else {
                    paper.style.display = 'none';
                }
            });
        });
    });
    
    // Venue filter functionality
    const filterButtons = document.querySelectorAll('.filter-btn');
    filterButtons.forEach(button => {
        button.addEventListener('click', function() {
            // Remove active class from all filter buttons
            filterButtons.forEach(btn => {
                btn.classList.remove('active');
                btn.style.backgroundColor = '#f5f7fa';
                btn.style.color = '#333';
                btn.style.border = '1px solid #e0e0e0';
            });
            
            // Add active class to clicked button
            this.classList.add('active');
            this.style.backgroundColor = '#1772d0';
            this.style.color = 'white';
            this.style.border = 'none';
            
            const filter = this.getAttribute('data-filter');
            const papers = document.querySelectorAll('.paper');
            
            papers.forEach(paper => {
                if (filter === 'all' || paper.getAttribute('data-venue') === filter) {
                    paper.style.display = 'flex';
                } else {
                    paper.style.display = 'none';
                }
            });
        });
    });

    // Add hover effects for buttons
    const allButtons = document.querySelectorAll('.year-btn, .filter-btn');
    allButtons.forEach(button => {
        button.addEventListener('mouseenter', function() {
            if (!this.classList.contains('active')) {
                this.style.backgroundColor = '#e9f0f8';
                this.style.transform = 'translateY(-2px)';
                this.style.boxShadow = '0 4px 8px rgba(0,0,0,0.1)';
            }
        });
        
        button.addEventListener('mouseleave', function() {
            if (!this.classList.contains('active')) {
                this.style.backgroundColor = '#f5f7fa';
                this.style.transform = 'translateY(0)';
                this.style.boxShadow = 'none';
            }
        });
    });
});
</script>
    <div class="section">
        <h2>Honors</h2>
        <div class="card">
            <ul>
                  <li>Ant InTech Scholarship–Future  2025</li>
                <li>DAAD NeT-AI Fellowship 2024</li>
                <li>Top 3 in CVPR2022 Second lightweight NAS challenge supernet Track, 2022</li>
                <li>Outstanding entrepreneurial undergraduate, 2019</li>
                <li>Second Prize (National-level) China Undergraduate electronic design competition, 2017, 2018</li>
                <li>MCM/ICM -- Honorable Mention, 2016</li>
            </ul>
        </div>
    </div>

    <div class="section">
        <h2>Services</h2>
        <div class="card">
            <h3>Area Chair</h3>
            <ul>
                <li>International Conference on Learning Representations (ICLR), 2026</li>
                     <li>Conference on Neural Information Processing Systems (NeurIPS), 2025</li>
                     <li>ACM International Conference on Multimedia (ACM-MM), 2025</li>
                  <li>2026 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2026</li>
                <li>International Joint Conference on Neural Networks (IJCNN), 2025</li>
                <li>British Machine Vision Conference (BMVC), 2024</li>
            </ul>

            <h3>Conference Review</h3>
            <ul>
                <li>2025: CVPR, ICML, IJCAI, AutoML, ACL</li>
                <li>2024: ACM MM, ECCV, ICML, NeurIPS, ICLR, AAAI, AutoML, ACL</li>
                <li>2023: CVPR, AAAI, ACM MM, ICCV, NeurIPS, ICLR, WACV, AutoML</li>
                <li>2022: CVPR, AAAI, ACM MM, ECCV, NeurIPS, WACV</li>
                <li>2021: AAAI, ACM MM</li>
            </ul>

            <h3>Journal Review</h3>
            <ul>
                <li>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</li>
                <li>International Journal of Computer Vision (IJCV)</li>
                <li>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</li>
                <li>IEEE Transactions on Circuits System and Video Technology (TCSVT)</li>
            </ul>
        </div>
    </div>

<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=7PmJikxJk324BBRWUrbE2SWNBd9X6PBJ1XTz2PMpA7k&cl=ffffff&w=a"></script>

    <script>
        // Get visitor IP and location info
        fetch('https://ipapi.co/json/')
            .then(response => response.json())
            .then(data => {
                document.getElementById('visitor-ip').textContent = data.ip;
                document.getElementById('visitor-location').textContent = `${data.city}, ${data.country_name}`;
                
                // Load map with visitor's location
                loadMap(data.latitude, data.longitude, data.city, data.country_name);
                
                // Set browser info
                const browserName = getBrowserName();
                document.getElementById('visitor-browser').textContent = browserName;
            })
            .catch(error => {
                console.error('Error fetching visitor info:', error);
                document.getElementById('visitor-ip').textContent = 'Error';
                document.getElementById('visitor-location').textContent = 'Error';
                
                // Load default map
                loadMap(22.3193, 114.1694, 'Hong Kong', 'China');
            });

        // Load map using Leaflet.js
        function loadMap(lat, lon, city, country) {
            // Add Leaflet CSS
            const leafletCSS = document.createElement('link');
            leafletCSS.rel = 'stylesheet';
            leafletCSS.href = 'https://unpkg.com/leaflet@1.9.3/dist/leaflet.css';
            document.head.appendChild(leafletCSS);
            
            // Add Leaflet JS
            const leafletScript = document.createElement('script');
            leafletScript.src = 'https://unpkg.com/leaflet@1.9.3/dist/leaflet.js';
            document.head.appendChild(leafletScript);
            
            // Initialize map when Leaflet is loaded
            leafletScript.onload = function() {
                const map = L.map('visitor-map').setView([lat, lon], 5);
                
                L.tileLayer('https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', {
                    attribution: '&copy; <a href="https://www.openstreetmap.org/copyright">OpenStreetMap</a> contributors'
                }).addTo(map);
                
                L.marker([lat, lon]).addTo(map)
                    .bindPopup(`Visitor location: ${city}, ${country}`)
                    .openPopup();
                
                // Add HKUST location marker
                const hkustLat = 22.3370;
                const hkustLon = 114.2639;
                
                L.marker([hkustLat, hkustLon]).addTo(map)
                    .bindPopup('HKUST: Hong Kong University of Science and Technology');
                
                // Add a line connecting visitor to HKUST
                const latlngs = [
                    [lat, lon],
                    [hkustLat, hkustLon]
                ];
                
                L.polyline(latlngs, {color: 'blue', dashArray: '5, 10'}).addTo(map);
            };
        }
        
        // Get browser name
        function getBrowserName() {
            const userAgent = navigator.userAgent;
            let browserName;
            
            if (userAgent.match(/chrome|chromium|crios/i)) {
                browserName = "Chrome";
            } else if (userAgent.match(/firefox|fxios/i)) {
                browserName = "Firefox";
            } else if (userAgent.match(/safari/i)) {
                browserName = "Safari";
            } else if (userAgent.match(/opr\//i)) {
                browserName = "Opera";
            } else if (userAgent.match(/edg/i)) {
                browserName = "Edge";
            } else {
                browserName = "Unknown";
            }
            
            return browserName;
        }
    </script>
</body>
</html>
